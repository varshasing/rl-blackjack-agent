{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshasing/rl-blackjack-agent/blob/main/Reinforcement_Learning_Blackjack_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCLr4xrwmuuL"
      },
      "source": [
        "## Learning to play Blackjack\n",
        "\n",
        "Implementation by Varsha Singh @ Boston University\n",
        "\n",
        "\n",
        "\n",
        "Blackjack is a card game played against a \"dealer\". The full rules, including various betting rules we will not use, are available online here: https://bicyclecards.com/how-to-play/blackjack/\n",
        "\n",
        "The object of the game is to collect a hand of cards whose point total is as close to 21 as possible *WITHOUT* going over. If your point total is over 21, then the dealer automatically wins. A hand of cards is scored as follows:\n",
        "\n",
        "Each card is worth a certain number of points. For a card specified by a `(value, suit)` tuple,\n",
        "the number of points it is worth is completely determined by the  `value` term in the tuple (`suit` is irrelevant).\n",
        "\n",
        "* If `value` is a number \"2\" through \"10\", then the card is worth that number of  points.\n",
        "* If `value` is \"Jack\", \"Queen\" or \"King\", then the card is worth 10 points.\n",
        "* If `value` is \"Ace\", then the card is worth EITHER 1 point or 11 points - the player may choose.\n",
        "\n",
        "The total score of a hand is the total number of points for all cards in the hand, counting each Ace in whichever manner (either 11 or 1) the player chooses.\n",
        "\n",
        "At the start of the game, you are dealt two cards, and the dealer is dealt  1 card that is visible to you. You may take one of  two actions: a \"HIT\" or  a \"STAY\". If you take the \"HIT\" action, you will be given another card. Getting another card will increase your score, but might also make your score go over 21. If you take the \"STAY\" action, this  indicates that you  are satisfied  with  your current point total (i.e. you suspect that taking another card might make your score go over  21, or that the dealer will not able to beat your  score without  going over 21 themselves). Not that you may adjust your decision to count an Ace as 1 point  or 11 points at  any  time. So, for  example, if you hand consists of a 9 of clubs and an Ace of spades, you might count the Ace as 11 points for a total of 20 points. However, if you then take the \"HIT\" action (which is very inadvisable in this case!), you might  be dealt an 8 of hearts, in which case your  total would be  either 28 (counting the Ace as 11)  or 18  (counting  the Ace as 1). So, you could now count the Ace as 1 point and choose the  \"STAY\" action, to indicate that you are no longer interested in taking another car.\n",
        "\n",
        "If you take the \"HIT\" action, then you are dealt a card and again have the option to either take a \"HIT\" or \"STAY\" action. You may continue taking the \"HIT\" action and getting cards until your total is over 21 no matter how you choose to count the Aces. If this happens, then your hand is called \"bust\" and you automatically lose. So, you should try to choose  the \"STAY\" action before this occurs.\n",
        "\n",
        "If you take the \"STAY\" action, then the dealer will start playing their hand.  The dealer is not allowed to strategize: the  dealer will continually take  the  \"HIT\" action until their hand is worth 17 points or more. Once that happens, the dealer takes the \"STAY\" action. The dealer must count Aces as  11 points unless counting the Ace this  way would  cause the  dealer's score to go  over 21.\n",
        "\n",
        "Outcome of the game:\n",
        "\n",
        "If you go bust, you automatically lose. If you do  not go bust,  but the dealer  does, then you win. If neither of you go bust, then the person with the  highest score  wins. Otherwise, it is a tie.\n",
        "\n",
        "\n",
        "Note that the player gets to see the  first card that  the dealer is dealt, but NOT any of the other cards that the dealer will take when making their decisions.\n",
        "\n",
        "\n",
        "### Reward Structure\n",
        "To fit this game into the reinforcement learning framework, we need to decide on what rewards to give the  player. We give rewards in the following way:\n",
        "\n",
        "* If the player takes an action that results in a loss (either \"STAY\" and then losing after the dealer finishes playing, or \"HIT\" and going bust), then the  reward is -1.\n",
        "\n",
        "* If the player takes an action that results in a win (i.e. a \"STAY\" followed by the dealer either going bust or  having a smaller score than the  player), then the reward is 1.\n",
        "\n",
        "* If the player takes an action that results in a tie (i.e. a \"STAY\" followed by the dealer achieving the same score as the player), then the reward is 0.\n",
        "\n",
        "* If the player takes  an action that does  not result in  the game ending (that is, a \"HIT\" with the possibility of taking another \"HIT\"), then the reward is 0.\n",
        "\n",
        "## Action Structure\n",
        "\n",
        "The  set of actions  available to the  player  is always either both \"HIT\" and \"STAY\", or nothing at all (which happens after taking the \"STAY\" action, or going  bust), both of which result in entering a terminal state, as the dealer will play out their hand and the player will take no more actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SejcV3lo8dZh"
      },
      "source": [
        "\n",
        "Blackjack simulator, converting 'full state' of the blackjack game into a 'minimal state' that contains just the critical information needed for a reinforcement learning agent to play the game optimally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w-Oo5IOmuFc"
      },
      "outputs": [],
      "source": [
        "# Two functions for running a blackjack game\n",
        "\n",
        "def draw_card():\n",
        "  '''\n",
        "  returns a randomly selected (value, suit) tuple representing a random card.\n",
        "  Assumes cards are drawn from infinitely many decks shuffled together (so\n",
        "  your agent cannot gain an advantage by counting cards in this game).\n",
        "  '''\n",
        "  value = random.choice(['Ace'] + [str(i) for i in range(2,11)] + ['Jack', 'Queen', 'King'])\n",
        "  suit = random.choice(['Clubs', 'Hearts', 'Spades', 'Diamonds'])\n",
        "\n",
        "  return (value, suit)\n",
        "\n",
        "def hand_to_str(hand):\n",
        "  '''\n",
        "  Returns a string containing a human-readable description of a blackjack hand.\n",
        "  Note this function returns a string, it does not call `print`.\n",
        "  '''\n",
        "  def card_to_str(c):\n",
        "    value, suit = c\n",
        "    return value + ' of ' + suit\n",
        "  return ', '.join([card_to_str(c) for c in hand])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwZSI4yebpIN"
      },
      "outputs": [],
      "source": [
        "def score_hand(hand):\n",
        "  '''\n",
        "  returns the best possible score for a given hand.\n",
        "\n",
        "  You can read the full scoring rules below, and also at this link https://bicyclecards.com/how-to-play/blackjack/ under \"Card Values/scoring\".\n",
        "\n",
        "  The rules for scoring are:\n",
        "  Each card is worth a certain number of points. For a card specified by a (value, suit) tuple,\n",
        "  the number of points it is worth is completely determined by the  \"value\" term in the tuple (suit is irrelevant).\n",
        "  If the value term is a number 2 through 10, then the card is worth that number of  points.\n",
        "  If the value is \"Jack\", \"Queen\" or \"King\", then the card is worth 10 points.\n",
        "  If the value is \"Ace\", then the card is worth EITHER 1 point or 11 points - the player may choose.\n",
        "\n",
        "  The total score of a hand is the total number of points for all cards in the hand, counting each Ace in whatever manner\n",
        "  the player chooses.\n",
        "  The objective of the game is to  have as high as score as possible WITHOUT going over 21. So a score of 21 is a \"perfect\" score,\n",
        "  a score of 20 is a very good score, and a score of 22 is very bad score (if you go over 21 you automatically lose).\n",
        "\n",
        "  arguments:\n",
        "    hand: a list of (value, suit) pairs specifying the hand of the player.\n",
        "        both \"value\" and \"suit\" are represented by strings.\n",
        "  return two values:\n",
        "    (score, eleven_valued_aces)\n",
        "\n",
        "    score: a non-negative integer representing either:\n",
        "      1. The highest score possible with this hand that is less than or equal to 21. Multiple scores  might be  possible  if the  hand\n",
        "        contains one or more  Aces. You  must find the  largest score that is less than or equal to 21 if such a thing exists.\n",
        "      2. If it is impossible for this hand to score less than or equal to 21 (the hand is called \"bust\" in this case),\n",
        "        then return the lowest possible score for the hand (which must necessarily be >21).\n",
        "\n",
        "    eleven_valued_aces: the number of aces in the hand that were scored at 11 points rather than 1 point\n",
        "      in order to achieve the score provided in the first return value (these are also called \"soft\" aces).\n",
        "    '''\n",
        "\n",
        "  # add all non-aces and for every ace, add 11 if under 21 : add 1\n",
        "  aces = 0\n",
        "  eleven_aces = 0\n",
        "  score = 0\n",
        "  for card in hand:\n",
        "    if card[0] == 'Ace':\n",
        "      aces += 1\n",
        "    elif card[0] == 'Jack' or card[0] == 'Queen' or card[0] == 'King':\n",
        "      score += 10\n",
        "    else:\n",
        "      score += int(card[0])\n",
        "  while aces > 0:\n",
        "    if score + 11 <= 21:\n",
        "      score += 11\n",
        "      eleven_aces += 1\n",
        "    else:\n",
        "      score += 1\n",
        "    aces -= 1\n",
        "  return score, eleven_aces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1go9NWdWw9O"
      },
      "outputs": [],
      "source": [
        "test_cases = {\n",
        "  (('Ace', 'Clubs'),): (11, 1),\n",
        "  (('Ace', 'Hearts'), ('Ace', 'Spades'),): (12, 1),\n",
        "  (('8', 'Clubs'), ('King', 'Diamonds'), ('9', 'Spades'),): (27, 0),\n",
        "  (('Ace', 'Hearts'), ('7', 'Diamonds'), ('Ace', 'Clubs'), ('King', 'Diamonds'),): (19, 0),\n",
        "  (('10', 'Spades'),): (10, 0),\n",
        "  (('King', 'Spades'), ('Ace', 'Clubs'),): (21, 1),\n",
        "  (('Ace', 'Hearts'), ('7', 'Diamonds'), ('10', 'Diamonds'),): (18, 0),\n",
        "  (('10', 'Clubs'), ('4', 'Hearts'), ('Ace', 'Spades'), ('8', 'Spades'),): (23, 0),\n",
        "  (('4', 'Clubs'), ('7', 'Hearts'), ('Ace', 'Spades'), ('2', 'Clubs'), ('King', 'Diamonds'),): (24, 0),\n",
        "  (('2', 'Hearts'), ('5', 'Hearts'), ('2', 'Hearts'), ('3', 'Hearts'), ('3', 'Diamonds'), ('King', 'Spades'),): (25, 0),\n",
        "  (('10', 'Clubs'),): (10, 0),\n",
        "  (('King', 'Hearts'), ('Ace', 'Spades'),): (21, 1),\n",
        "  (('4', 'Spades'), ('5', 'Diamonds'), ('3', 'Hearts'),): (12, 0),\n",
        "  (('4', 'Diamonds'), ('8', 'Hearts'), ('4', 'Spades'), ('6', 'Diamonds'),): (22, 0),\n",
        "  (('Jack', 'Spades'),): (10, 0),\n",
        "  (('10', 'Diamonds'), ('7', 'Clubs'),): (17, 0),\n",
        "  (('Jack', 'Hearts'), ('6', 'Clubs'), ('6', 'Spades'),): (22, 0),\n",
        "  (('2', 'Diamonds'), ('Ace', 'Spades'), ('8', 'Spades'), ('7', 'Spades'),): (18, 0),\n",
        "  (('King', 'Clubs'),): (10, 0),\n",
        "  (('Ace', 'Hearts'), ('Jack', 'Diamonds'),): (21, 1),\n",
        "  (('4', 'Clubs'), ('2', 'Diamonds'), ('4', 'Spades'),): (10, 0),\n",
        "  (('King', 'Spades'), ('10', 'Hearts'), ('2', 'Clubs'), ('5', 'Spades'),): (27, 0),\n",
        "  (('Queen', 'Hearts'),): (10, 0),\n",
        "}\n",
        "\n",
        "def test_score_hand():\n",
        "  for hand, true_output in test_cases.items():\n",
        "    output = score_hand(list(hand))\n",
        "    assert output == true_output, f\"incorrect output on test case: {hand}. Expected output: {true_output}, computed output: {output}\"\n",
        "  print(\"all tests pass :)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh1KvVXPYdUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2ed736-aa6c-49bd-c6da-6e672e483241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all tests pass :)\n"
          ]
        }
      ],
      "source": [
        "# test scoring function\n",
        "test_score_hand()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xcq91icwD5e"
      },
      "source": [
        "The following cell contains the blackjack game code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaDEsb0VMJIt"
      },
      "outputs": [],
      "source": [
        "class BlackjackSimulator:\n",
        "  '''\n",
        "  This class simulates a (slightly simplified) version of the card game Blackjack.\n",
        "\n",
        "  The player's current hand is accessible via the `player_hand` attribute.\n",
        "  The dealers' current hand is accessible via the `dealer_hand` attribute.\n",
        "\n",
        "  Both hands are stored as lists of cards, where each card is a tuple (value, suit) as\n",
        "  returned by `draw_card()`\n",
        "\n",
        "  The actions currently available to the player are accessible via `available_actions` attribute.\n",
        "  The last reward earned by the player is accessible via the `reward` attribute.\n",
        "\n",
        "  Check the docstrings of the individual methods to see what they do.\n",
        "  In our reference solution, we access the `player_hand` and `dealer_hand` attribute\n",
        "  in the function `get_state`. We do not use any other internals of BlackjackSimulator objects.\n",
        "  However, if you find it useful to use some internal methods or attributes, you may do so.\n",
        "\n",
        "  See the rules of blackjack here: https://bicyclecards.com/how-to-play/blackjack/\n",
        "\n",
        "  We do not play with \"naturals\" here as they do not require any decision making.\n",
        "  This actually makes the odds a little worse than they should be, so your agent\n",
        "  would actually do slightly better in a casino than it will in this simulator.\n",
        "\n",
        "  It still wouldn't beat the house though. Gambling is dangerous, and we do not endorse it!\n",
        "\n",
        "  If you find any other discrepencies in the official rules and the game as implemented\n",
        "  in this simulator, please write your code to do well on this simulator, not the official rules :)\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    # the player starts with two cards\n",
        "    self.player_hand = [draw_card(), draw_card()]\n",
        "\n",
        "    # the dealer starts with one visible card\n",
        "    self.dealer_hand = [draw_card()]\n",
        "\n",
        "    self.available_actions = ['HIT', 'STAY']\n",
        "\n",
        "    # this will hold the reward of the current state of the game. It will always be\n",
        "    # either -1, (loss), 0 (game in progress, or tie), or 1 (win)\n",
        "    self.reward = 0\n",
        "\n",
        "\n",
        "  def is_game_over(self):\n",
        "    '''\n",
        "    returns True if the game is over.\n",
        "    At this point, self.reward will hold the result of the game\n",
        "    (0 for tie, 1 for player win, -1 for player loss).\n",
        "    '''\n",
        "    return len(self.available_actions) == 0\n",
        "\n",
        "  def player_hand_str(self):\n",
        "    '''\n",
        "    returns a string representation of the player hand (it does not actually\n",
        "    call the `print` method).\n",
        "    '''\n",
        "    return hand_to_str(self.player_hand)\n",
        "\n",
        "  def dealer_hand_str(self):\n",
        "    '''\n",
        "    returns a string representation of the dealer hand (it does not actually\n",
        "    call the `print` method).\n",
        "    '''\n",
        "    return hand_to_str(self.dealer_hand)\n",
        "\n",
        "  def get_reward(self):\n",
        "    '''\n",
        "    get reward from last action taken.\n",
        "    '''\n",
        "    self.reward = self._get_reward()\n",
        "    return self.reward\n",
        "\n",
        "  def _get_reward(self):\n",
        "    '''\n",
        "    helper function for self.get_reward: this one has all the actual logic.\n",
        "    '''\n",
        "    if len(self.available_actions) != 0:\n",
        "      # we are still playing - you might yet win!\n",
        "      return 0\n",
        "\n",
        "    dealer_score = self.dealer_score()\n",
        "    player_score = self.player_score()\n",
        "    if player_score==dealer_score:\n",
        "      # this is the only way to tie.\n",
        "      return 0\n",
        "\n",
        "    # we must have a winner.\n",
        "    if player_score > 21:\n",
        "      # dealer wins when player busts (even if the dealer also busts)\n",
        "      return -1\n",
        "    if dealer_score > 21:\n",
        "      # dealer bust, player did not bust.\n",
        "      return 1\n",
        "    if player_score > dealer_score:\n",
        "      return 1\n",
        "    return -1\n",
        "\n",
        "  def player_score(self):\n",
        "    return score_hand(self.player_hand)[0]\n",
        "\n",
        "  def dealer_score(self):\n",
        "    return score_hand(self.dealer_hand)[0]\n",
        "\n",
        "  def take_action(self, action: str):\n",
        "    '''\n",
        "    plays one turn of blackjack.\n",
        "\n",
        "    arguments:\n",
        "      action: a string value, must be an element of self.available_actions\n",
        "\n",
        "    returns:\n",
        "      a reward for this action (integer).\n",
        "    '''\n",
        "\n",
        "    assert action in self.available_actions, f\"Attempted action: {action}, but the available actions are: {self.available_actions}. The current hand is: {self.player_hand()}\"\n",
        "\n",
        "\n",
        "    if action == 'STAY':\n",
        "      self.available_actions = []\n",
        "      # once the player STAYs, the dealer will play out its hand.\n",
        "      while self.dealer_score() < 17:\n",
        "        # dealer hits until score is 17 or bigger\n",
        "        self.dealer_hand.append(draw_card())\n",
        "    else:\n",
        "      self.player_hand.append(draw_card())\n",
        "      if self.player_score() > 21:\n",
        "        # BUST!\n",
        "        self.available_actions = []\n",
        "\n",
        "\n",
        "    return self.get_reward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2HhA6rfawyA"
      },
      "outputs": [],
      "source": [
        "## This cell contains some helper functions that print the current state of the\n",
        "# blackjack game in a human-readable way, as well as allow you to test the blackjack\n",
        "# simulator by playing with it yourself.\n",
        "\n",
        "\n",
        "def print_status(bs):\n",
        "  print(f\"Current Total: {bs.player_score()}\\nCurrent hand: {bs.player_hand_str()}\")\n",
        "  print(f\"Dealer Total: {bs.dealer_score()}\\nDealer hand: {bs.dealer_hand_str()}\")\n",
        "  if len(bs.available_actions) == 0:\n",
        "    if bs.get_reward() == 0:\n",
        "      print(f\"It's a tie!\")\n",
        "    elif bs.get_reward() == 1:\n",
        "      print(f\"Player Wins!\")\n",
        "    elif bs.get_reward() == -1:\n",
        "      print(f\"Player loses!\")\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def play_blackjack():\n",
        "  bs = BlackjackSimulator()\n",
        "\n",
        "  def get_action():\n",
        "    action = None\n",
        "    options = bs.available_actions\n",
        "    while action not in options:\n",
        "      if  action is not None:\n",
        "        print(f\"{action} is not an allowed action. Please select again.\")\n",
        "      print(\"Available Actions:\")\n",
        "      for option in options:\n",
        "        print(f\"* {option}\")\n",
        "      action = input(\"Your selection: \").upper()\n",
        "    return action\n",
        "\n",
        "  print_status(bs)\n",
        "  while not bs.is_game_over():\n",
        "    action = get_action()\n",
        "    bs.take_action(action)\n",
        "    print_status(bs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxG0PhWjwR4X"
      },
      "source": [
        "If you want, run following cell to play blackjack yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxx4pIgycoA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56be1c2-1696-4d0c-a595-72b98e7d6f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Total: 14\n",
            "Current hand: 3 of Clubs, Ace of Spades\n",
            "Dealer Total: 10\n",
            "Dealer hand: Queen of Spades\n",
            "Available Actions:\n",
            "* HIT\n",
            "* STAY\n",
            "Your selection: HIT\n",
            "Current Total: 15\n",
            "Current hand: 3 of Clubs, Ace of Spades, Ace of Clubs\n",
            "Dealer Total: 10\n",
            "Dealer hand: Queen of Spades\n",
            "Available Actions:\n",
            "* HIT\n",
            "* STAY\n",
            "Your selection: HIT\n",
            "Current Total: 19\n",
            "Current hand: 3 of Clubs, Ace of Spades, Ace of Clubs, 4 of Hearts\n",
            "Dealer Total: 10\n",
            "Dealer hand: Queen of Spades\n",
            "Available Actions:\n",
            "* HIT\n",
            "* STAY\n",
            "Your selection: STAY\n",
            "Current Total: 19\n",
            "Current hand: 3 of Clubs, Ace of Spades, Ace of Clubs, 4 of Hearts\n",
            "Dealer Total: 26\n",
            "Dealer hand: Queen of Spades, 6 of Clubs, 10 of Spades\n",
            "Player Wins!\n"
          ]
        }
      ],
      "source": [
        "play_blackjack()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA3W88Wb35so"
      },
      "source": [
        "`get_state` captures the minimal information needed by an agent to make an optimal decision about what action to take. As specified in the docstring, the input should be a BlackjackSimulator object as defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIt3LndiZsxE"
      },
      "outputs": [],
      "source": [
        "def get_state(sim: BlackjackSimulator):\n",
        "  '''\n",
        "  find the score and number of aces values at 11 points rather than 1 point in both the player and dealer's hands (aka \"soft aces\").\n",
        "\n",
        "  arguments:\n",
        "    sim: a BlackjackSimulator object.\n",
        "\n",
        "  returns:\n",
        "     a tuple (player_score, player_eleven_valued_aces, dealer_score, dealer_eleven_valued_aces)\n",
        "  '''\n",
        "\n",
        "  # extract a tuple (player_score, player_eleven_valued_aces, dealer_score, dealer_eleven_valued_aces)\n",
        "  player_score, player_eleven_valued_aces = score_hand(sim.player_hand)\n",
        "  dealer_score, dealer_eleven_valued_aces = score_hand(sim.dealer_hand)\n",
        "  return player_score, player_eleven_valued_aces, dealer_score, dealer_eleven_valued_aces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNhJJF9u9Nz5"
      },
      "outputs": [],
      "source": [
        "# Argument checking functions that are useful for making sure implementation\n",
        "# of Q-learning is passing the correct values around.\n",
        "\n",
        "def assert_valid_action(action):\n",
        "  assert action in ['HIT', 'STAY'], f\"invalid action: {action}\"\n",
        "\n",
        "def assert_valid_state(state):\n",
        "  assert type(state) == tuple, f\"state must be a tuple, but was: {state}\"\n",
        "\n",
        "  p_score, p_aces, d_score, d_aces = state\n",
        "  assert type(p_score) == int and type(p_aces) == int and type(d_score) == int and type(d_aces) == int, f\"state has invalid types: {state}\"\n",
        "  assert p_score>=4 and p_aces>=0 and p_score<=31 and d_score>=2 and d_aces>=0 and d_score<=27, f\"state has impossible values: {state}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVUZyNWk9tri"
      },
      "source": [
        "\n",
        "The output of `get_state` is sufficient for designing an optimal agent because using the full state of 52 different cards (up to) requires a lot more space and time for computations. When playing the game, I thought moreso about how strong the dealers hand is and compared it to mine, as well as the amount of flexibility I had with aces. It wasn't really a consideration of mine to think about the different permutations of amounting to a score, as there are 4 different ways of having a card be a value of 2, So, I believe that the neccessary information for an optimal agent shouldn't worry about the different suites and rather worry about the numerical scores and how much flexibility they have with their aces, as they can be two-valued."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emcxvjHC9JF2"
      },
      "outputs": [],
      "source": [
        "class Q_Function:\n",
        "  '''\n",
        "  class for holding a Q function.\n",
        "  This class is essentially a manual implementation of a DefaultDict with\n",
        "  default value zero and a little type checking on top to help catch some bugs\n",
        "  that might occur when you use it.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.Q = dict()\n",
        "\n",
        "  def set_value(self, state, action, value):\n",
        "    '''\n",
        "    sets the estimated Q(s, a) to be a given value.\n",
        "\n",
        "    arguments:\n",
        "      state: the state (a tuple as returned  by get_state)\n",
        "      action:  a string.\n",
        "      value: a float.\n",
        "\n",
        "    returns\n",
        "      None\n",
        "\n",
        "    This function updates the Q function so that Q(state, action) = value.\n",
        "    '''\n",
        "\n",
        "    assert_valid_state(state)\n",
        "    assert_valid_action(action)\n",
        "    assert value <=1.00001 and value >= -1.00001, f\"Q learning should never set a Q value of: {value}!\"\n",
        "    if state not in self.Q:\n",
        "      self.Q[state] = {}\n",
        "    self.Q[state][action] = value\n",
        "\n",
        "  def get_value(self, state, action):\n",
        "    '''\n",
        "    Return Q(s,a): gets the estimated Q value for a given state, action pair.\n",
        "    '''\n",
        "    assert_valid_state(state)\n",
        "    assert_valid_action(action)\n",
        "    if not self.seen(state, action):\n",
        "      return 0.0\n",
        "    return self.Q[state][action]\n",
        "\n",
        "\n",
        "  ## the following functions are  not needed, but might be useful\n",
        "  ## in debugging\n",
        "  def get_seen_actions_for_state(self, state):\n",
        "    '''\n",
        "    for a given state, return a list of all actions that have been \"seen\"\n",
        "    in the  sense that the agent has taken this action from this state.\n",
        "    '''\n",
        "    assert_valid_state(state)\n",
        "    if state not in self.Q:\n",
        "      return []\n",
        "    return self.Q[state].keys()\n",
        "\n",
        "  def seen(self, state, action):\n",
        "    '''\n",
        "    Return True if the given  state, action pair has been visited by the agent before.\n",
        "    False otherwise.\n",
        "    '''\n",
        "    assert_valid_state(state)\n",
        "    assert_valid_action(action)\n",
        "    if state not in self.Q:\n",
        "      return False\n",
        "    if action not in self.Q[state]:\n",
        "      return False\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbhffb6s9wHP"
      },
      "source": [
        "## Implement Q-Learning.\n",
        "\n",
        "### Notes on Q Learning.\n",
        "\n",
        "Q-learning allows us to train a policy on-the-fly while playing many games of Blackjack. We maintain \"Q-function\": $Q(s,a)$. Eventually $Q(s,a)$ should equal:\n",
        "$$\n",
        "Q(s,a) = r(s,a) + \\sum_{s'} P(s'|s,a) V_{\\pi_\\star}(s')\n",
        "$$\n",
        "That is, $Q(s,a)$ records the total reward we can expect to obtain if we (1) start in state $s$, and then (2) take action $a$, and finally (3) follow the optimal policy $\\pi_\\star$ for all subsequent actions. If we knew this $Q$ function, exactly, then it would hold that $\\pi_\\star(s) = \\text{argmax}_a Q(s,a)$.\n",
        "\n",
        "Furthermore, the correct $Q$ function should satisfy the following identities:\n",
        "$$\n",
        "\\begin{align*}\n",
        "V_{\\pi_\\star}(s) &= \\max_a Q(s,a)\\\\\n",
        "Q(s,a) &= r(s,a) + \\sum_{s'} P(s'|s,a) \\max_a Q(s', a)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Q-learning does the following: we maintain an estimate of $Q(s,a)$ for all states/actions $s,a$. (i.e. initialized as $Q(s,a)=0$ to begin). Our estimate will be *incorrect* throughout the learning process, but will slowly converge to the true value. Now, when in state $s_t$, the learner will:\n",
        "\n",
        "1. Take action $a_t=\\text{argmax}_{a} Q(s_t,a)$.\n",
        "2. See reward $r_t=r(s,a)$, transition to state $s_{t+1}$.\n",
        "3. Update the value of $Q(s_t,a_t)$ via the formula:\n",
        "$$\n",
        "Q(s_t, a_t)\\leftarrow Q(s_t, a_t) + \\eta \\cdot (\\max_a Q(s_{t+1}, a) + r_t - Q(s_t, a_t))\n",
        "$$\n",
        "where $\\eta$ is some provided learning rate.\n",
        "\n",
        "Intuitively, in step (1), we take an action \"assuming\" that our current estimate $Q$ is the \"correct\" $Q$ function (which is not actually true, but might become more true over time). Then, in step 3, we use our observed reward and state transition to update our estimate of $Q(s_t,a_t)$ to be hopefully more correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aZKrDeIj995"
      },
      "outputs": [],
      "source": [
        "\n",
        "def max_q_for_state(Q, state, actions):\n",
        "  '''\n",
        "  finds the best action to take in a given state according to the current estimated Q function.\n",
        "\n",
        "  arguments:\n",
        "  Q:  a Q_function object (see  definition above)\n",
        "  state: a tuple (player_score, player_aces, dealer_score, dealer_aces) as returned by get_state.\n",
        "  actions: available actions in this state.\n",
        "\n",
        "  returns:\n",
        "  (max_value, best_action)\n",
        "\n",
        "  max_value: the maximum Q function value associated with any action in this state.\n",
        "  best_action: the  action that gives the maximum Q function value from this state.\n",
        "  '''\n",
        "\n",
        "  # return max_value and best_action\n",
        "  max_value = -np.inf\n",
        "  best_action = None\n",
        "\n",
        "  for action in actions:\n",
        "    value = Q.get_value(state, action)\n",
        "    if value > max_value:\n",
        "      max_value = value\n",
        "      best_action = action\n",
        "  return max_value, best_action\n",
        "  raise NotImplementedError\n",
        "\n",
        "\n",
        "class Q_Learner:\n",
        "  '''\n",
        "  A Q-learning class.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, eta):\n",
        "    '''\n",
        "    eta: the learning rate for Q learning.\n",
        "    '''\n",
        "    self.Q = Q_Function()\n",
        "    self.eta = eta\n",
        "\n",
        "  def update(self, prev_state, prev_action, sim, reward):\n",
        "    '''\n",
        "    Updates the estimated Q function (self.Q) after taking\n",
        "    action `prev_action` in state `prev_state` and seeing reward  `reward`.\n",
        "    `sim` holds the BlackjackSimulator object that can  be used  to determine the\n",
        "    current state after taking action `prev_action`.\n",
        "\n",
        "    arguments:\n",
        "      prev_state: the previous state of the game (a tuple as returned  by get_state)\n",
        "      prev_action: the action previously taken by the player.\n",
        "      sim: a BlackjackSimulator object.\n",
        "      reward: the reward obtained by taking prev_action in prev_state.\n",
        "\n",
        "    returns:\n",
        "      None.\n",
        "\n",
        "    This function has no return value, but should update self.Q using the Q-learning\n",
        "    update rule.\n",
        "    '''\n",
        "\n",
        "    assert_valid_state(prev_state)\n",
        "    assert_valid_action(prev_action)\n",
        "\n",
        "    current_state = get_state(sim)\n",
        "    max_value, best_action = max_q_for_state(self.Q, current_state, [\"HIT\", \"STAY\"])\n",
        "\n",
        "    current_q = self.Q.get_value(prev_state, prev_action)\n",
        "\n",
        "    self.Q.set_value(prev_state, prev_action, current_q + self.eta * (reward + max_value - current_q))\n",
        "\n",
        "\n",
        "  def get_action(self, sim):\n",
        "    '''\n",
        "    find the action this agent will take given the current game state as\n",
        "    specified by the BlackjackSimulator object `sim` and the current estimated\n",
        "    Q function stored in `self.Q`.\n",
        "\n",
        "    arguments:\n",
        "      sim: a BlackjackSimulator object containing the current game state.\n",
        "\n",
        "    returns:\n",
        "      action: a string representing the action to take next (i.e. either \"HIT\" or \"STAY\")\n",
        "    '''\n",
        "\n",
        "    state = get_state(sim)\n",
        "    actions = [\"HIT\", \"STAY\"]\n",
        "\n",
        "    max_value, best_action = max_q_for_state(self.Q, state, actions)\n",
        "    return best_action\n",
        "    raise NotImplementedError\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUTs1DYengTo"
      },
      "outputs": [],
      "source": [
        "def run_Q_learning(iter_count, eta):\n",
        "  policy = Q_Learner(eta)\n",
        "  pbar = tqdm(range(iter_count))\n",
        "  for i in pbar:\n",
        "    sim = BlackjackSimulator()\n",
        "    while not sim.is_game_over():\n",
        "      state = get_state(sim)\n",
        "      action = policy.get_action(sim)\n",
        "      reward = sim.take_action(action)\n",
        "      policy.update(state, action, sim, reward)\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsxXh_thqGXi"
      },
      "outputs": [],
      "source": [
        "def run_policy_verbose(learner, verbose_level=1):\n",
        "  '''\n",
        "  Run a learned policy on a single game of blackjack.\n",
        "  Prints out the states and actions taken during the game.\n",
        "  You may control the amount of information printed by increasing `verbose_level`.\n",
        "  '''\n",
        "  sim = BlackjackSimulator()\n",
        "  print_status(sim)\n",
        "  while not sim.is_game_over():\n",
        "    action = learner.get_action(sim)\n",
        "    state = get_state(sim)\n",
        "\n",
        "    print(f\"Learner takes action: {action}\")\n",
        "\n",
        "    if verbose_level > 0:\n",
        "      seen_actions = learner.Q.get_seen_actions_for_state(state)\n",
        "      print(f\"Actions tested in training: {seen_actions}\")\n",
        "      for seen_action in seen_actions:\n",
        "        print(f\"Q value for action {seen_action}: {learner.Q.get_value(state, seen_action)}\")\n",
        "\n",
        "    sim.take_action(action)\n",
        "\n",
        "    print_status(sim)\n",
        "\n",
        "  return sim.reward\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-4IAfVzGGYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9c5a95-62a1-414f-bb42-e688dc5acc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 7510.75it/s]\n"
          ]
        }
      ],
      "source": [
        "# This cell and the next one are to test the learner with a small number\n",
        "# of iterations\n",
        "small_policy = run_Q_learning(100, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfmhO8bAtiVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2403bd18-4e69-4fa1-fa01-3cc350e94cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Total: 20\n",
            "Current hand: King of Clubs, 10 of Spades\n",
            "Dealer Total: 3\n",
            "Dealer hand: 3 of Clubs\n",
            "Learner takes action: HIT\n",
            "Actions tested in training: []\n",
            "Current Total: 30\n",
            "Current hand: King of Clubs, 10 of Spades, Queen of Clubs\n",
            "Dealer Total: 3\n",
            "Dealer hand: 3 of Clubs\n",
            "Player loses!\n"
          ]
        }
      ],
      "source": [
        "reward = run_policy_verbose(small_policy, verbose_level=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP0IceSlGf9-"
      },
      "source": [
        "## Evaluating Q-learning algorithm.\n",
        "If performance is worse than -0.07, then you probably have a bug in an earlier question. If your performance is better than -0.05, then you also probably have a bug in an earlier question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbd1qnihsATz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a40e20-ab6a-4b67-e9e8-be69d51c8ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200000/200000 [00:13<00:00, 14479.88it/s]\n"
          ]
        }
      ],
      "source": [
        "# let's train for 200000 games of blackjack. If a real game took about 5 minutes, this would be almost two years of continuous blackjack!\n",
        "# This should not take very long\n",
        "policy = run_Q_learning(200000, 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98HZJhES4_GL"
      },
      "outputs": [],
      "source": [
        "# Testing code:\n",
        "def run_policy(learner):\n",
        "  sim = BlackjackSimulator()\n",
        "  while not sim.is_game_over():\n",
        "    action = learner.get_action(sim)\n",
        "    state = get_state(sim)\n",
        "    sim.take_action(action)\n",
        "\n",
        "  return sim.reward\n",
        "\n",
        "\n",
        "\n",
        "def test_policy(policy, iterations=10000):\n",
        "  '''\n",
        "  tests a policy for `iterations` number of games.\n",
        "  Returns the average reward obtained by the policy.\n",
        "  '''\n",
        "  pbar = tqdm(range(iterations))\n",
        "  total_reward = 0.0\n",
        "  for i in pbar:\n",
        "    total_reward += run_policy(policy)\n",
        "\n",
        "  return total_reward/iterations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbOrRKIt6pzc"
      },
      "source": [
        "Run this final cell to test implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSwehGEC5h20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c24586-5693-43f4-ca12-16673301a35d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 23152.84it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0578"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "# Test your learned policy. You should obtain a score of at least -0.07, and at most -0.05\n",
        "# The house always wins eventually :)\n",
        "\n",
        "test_policy(policy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}